{"cells":[{"cell_type":"code","execution_count":24,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1682776793393,"user":{"displayName":"Kilian Joss","userId":"11253052209737395074"},"user_tz":-120},"id":"9DXBA3gV5hQK"},"outputs":[],"source":["import pandas as pd"]},{"cell_type":"markdown","metadata":{},"source":["# prepare dataframe"]},{"cell_type":"code","execution_count":25,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1682776793393,"user":{"displayName":"Kilian Joss","userId":"11253052209737395074"},"user_tz":-120},"id":"TkI6NmWz5R8U"},"outputs":[],"source":["\n","df = pd.read_csv(r\"data\\d.csv\", encoding=\"utf-8\", encoding_errors='ignore')\n","# Dataset is now stored in a Pandas Dataframe"]},{"cell_type":"code","execution_count":26,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1682776793393,"user":{"displayName":"Kilian Joss","userId":"11253052209737395074"},"user_tz":-120},"id":"si29wK5UGYSP"},"outputs":[],"source":["df.drop(['count', 'offensive_language','neither','hate_speech'], axis = 1, inplace = True)\n","df.drop(df.columns[df.columns.str.contains('unnamed',case = False)],axis = 1, inplace = True)"]},{"cell_type":"code","execution_count":27,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":423},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1682776793393,"user":{"displayName":"Kilian Joss","userId":"11253052209737395074"},"user_tz":-120},"id":"Oxnm4p4j8juV","outputId":"9a13bba7-ae2d-4022-b7ac-b6deb7e79bfc"},"outputs":[],"source":["df_first = df[\"class\"].transform(lambda x: x.replace([1,2], \"XX\"))\n","df[\"class\"] = df_first.transform(lambda x: x.replace(0,1).replace(\"XX\",0)) #hate-speach = 1 in class everitying else  = 0"]},{"cell_type":"markdown","metadata":{},"source":["# split data"]},{"cell_type":"code","execution_count":29,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1682782818110,"user":{"displayName":"Kilian Joss","userId":"11253052209737395074"},"user_tz":-120},"id":"XC8_BkLhqt9G"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","X_data = df['tweet']\n","traget_class = df['class']\n","\n","\n","X_train, X_test , y_train, y_test = train_test_split(X_data, traget_class , test_size = 0.2)"]},{"cell_type":"code","execution_count":30,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1682776799908,"user":{"displayName":"Kilian Joss","userId":"11253052209737395074"},"user_tz":-120},"id":"uBU1D756I4iY"},"outputs":[],"source":["import re \n","\n"," \n","def preprocessing(tweet):\n","\n","  #use prepro of glove stanford 4 tweets https://nlp.stanford.edu/projects/glove/preprocess-twitter.rb\n","\n","   \n","\n","  #done  replace @user with \"USERNAME\"\n","  # handel emoji \n","  #done replace all links with \"LINK\"\n","  # delete ! and points\n","  # lower\n","\n","\n","  re_username = '@[\\w\\-]+'\n","  re_exclamation = \"\"\n","  re_emoji =\"\" # delete or not\n","  re_link = ('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|'\n","        '[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n","  re_all_words = \"[^a-z ]+\"\n","\n","\n","  tweet=re.sub(re_username,\"username\", tweet)\n","  tweet=re.sub(re_link,\"link\", tweet)\n","  tweet = tweet.lower() # only lower characters maybe better maybe worse\n","  tweet = re.sub(re_all_words,\" \",tweet) #space beacuse otherwise u merge some words\n","\n","\n","\n","  #read article if which stamming to use whit glove standford https://www.kaggle.com/general/218317\n","  tweet = tweet #stamming maybe addon if perforamnce bad\n","\n","  \n","\n","  return tweet"]},{"cell_type":"code","execution_count":31,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2022,"status":"ok","timestamp":1682776801925,"user":{"displayName":"Kilian Joss","userId":"11253052209737395074"},"user_tz":-120},"id":"MJsKpkKpdqTq","outputId":"cb34fda7-a0be-4cff-9bcc-5f761d918690"},"outputs":[{"data":{"text/plain":["\"import nltk\\nfrom nltk.corpus import stopwords\\nnltk.download('stopwords')\\nnltk.download('punkt')\\nfrom nltk.tokenize import word_tokenize\\n\\n \\ndef tokenizer_word_tokenize(tweet):\\n  tokens_tweet = word_tokenize(tweet)\\n  tokens_without_sw = [word for word in tokens_tweet if not word in stopwords.words()]\\n\\n  return tokens_tweet\""]},"execution_count":31,"metadata":{},"output_type":"execute_result"}],"source":[]},{"cell_type":"code","execution_count":32,"metadata":{"id":"MBWqcPHfdqNJ"},"outputs":[],"source":["# code for Glove word embedding\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","import numpy as np\n","  \n","\n","    \n","\n","def glove_embedings(x):\n","\n","    #print(\"X----------->\",x)\n","\n","    x = {x}\n","\n","\n","    tokenizer = Tokenizer()\n","    tokenizer.fit_on_texts(x)\n","\n","\n","\n","\n","    def embedding_for_vocab(filepath, word_index, embedding_dim):\n","\n","        vocab_size = len(word_index) + 1\n","        \n","        # Adding again 1 because of reserved 0 index\n","        embedding_matrix_vocab = np.zeros((vocab_size, embedding_dim))\n","    \n","        with open(filepath, encoding=\"utf8\") as f:\n","            for line in f:\n","                word, *vector = line.split()\n","                if word in word_index:\n","                    idx = word_index[word]\n","                    embedding_matrix_vocab[idx] = np.array(\n","                        vector, dtype=np.float32)[:embedding_dim]\n","    \n","        return embedding_matrix_vocab\n","    \n","    \n","    # matrix for vocab: word_index\n","    embedding_dim = 100\n","    embedding_matrix_vocab = embedding_for_vocab('data\\glove.twitter.27B.100d.txt', tokenizer.word_index, embedding_dim)\n","    \n","    #print(embedding_matrix_vocab[1])\n","\n","    return embedding_matrix_vocab"]},{"cell_type":"code","execution_count":33,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":623},"executionInfo":{"elapsed":5,"status":"error","timestamp":1682708965558,"user":{"displayName":"Kilian Joss","userId":"11253052209737395074"},"user_tz":-120},"id":"R7lSnaXgI4gI","outputId":"2a279956-5120-4f23-96f5-be195a873db1"},"outputs":[],"source":["def global_preprocessing(data):\n","\n","\n","    #handling emojis\n","    #df[\"tweet\"] = df[\"tweet\"].transform(lambda x: transform_emojis(x))\n","    \n","    \n","    data=data[:3] #df cutted for faster testing \n","    print(type(data))\n","    print(data)\n","\n","\n","    #print(preprocessing(\"test\"))\n","\n","    data = data.transform(lambda x: preprocessing(x))\n","\n","    data = data.transform(lambda x: glove_embedings(x))\n","\n","\n","    return data"]},{"cell_type":"code","execution_count":34,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'pandas.core.series.Series'>\n","11696    If your twitter is private but your Instagram ...\n","4507     @RealWallieWall Sheet i can't do that too a cr...\n","23835              fuck ray rice ,someone crack that bitch\n","Name: tweet, dtype: object\n","<class 'pandas.core.series.Series'>\n","11696    if your twitter is private but your instagram ...\n","4507          username sheet i can t do that too a cripple\n","23835              fuck ray rice  someone crack that bitch\n","Name: tweet, dtype: object\n","X-----------> if your twitter is private but your instagram is public u r probs a trash talking thot\n","{'your': 1, 'is': 2, 'if': 3, 'twitter': 4, 'private': 5, 'but': 6, 'instagram': 7, 'public': 8, 'u': 9, 'r': 10, 'probs': 11, 'a': 12, 'trash': 13, 'talking': 14, 'thot': 15}\n","Number of unique words in dictionary= 15\n","Dictionary is =  {'your': 1, 'is': 2, 'if': 3, 'twitter': 4, 'private': 5, 'but': 6, 'instagram': 7, 'public': 8, 'u': 9, 'r': 10, 'probs': 11, 'a': 12, 'trash': 13, 'talking': 14, 'thot': 15}\n","Dense vector for first word is =>  16\n","[-7.01289997e-02 -4.96569984e-02  5.39619982e-01  8.34140003e-01\n","  8.20670009e-01  2.48870000e-01  4.56369996e-01 -3.45499992e-01\n","  3.44040006e-01 -9.56489965e-02  4.66220006e-02  2.52779990e-01\n"," -6.14989996e+00 -5.28500021e-01 -5.67830026e-01  3.12050015e-01\n"," -1.54709995e-01 -1.77190006e-01 -2.14739993e-01  8.41960013e-02\n"," -3.54070008e-01  2.61119992e-01  2.44740006e-02  2.98449993e-01\n"," -7.10529983e-01  1.99110001e-01  4.95680004e-01  6.62119985e-01\n","  8.35879982e-01 -1.56709999e-01 -3.88110012e-01  4.37269986e-01\n"," -5.63889980e-01 -2.38110006e-01  5.73840022e-01  2.13310003e-01\n","  1.96999997e-01  1.24150001e-01 -4.05200005e-01 -6.97449982e-01\n"," -6.96439981e-01  2.49760002e-01  8.89599979e-01  4.34439987e-01\n","  1.18640006e+00 -1.26870006e-01  5.58260024e-01 -2.35970002e-02\n","  1.76880002e-01 -1.34310007e-01  2.80409992e-01  7.05479980e-02\n","  4.06630002e-02 -1.66590005e-01  8.75320017e-01  6.14950001e-01\n"," -3.14960003e-01  2.56239995e-02 -7.07160011e-02  1.80329993e-01\n","  3.60190004e-01 -2.54339993e-01 -3.11499997e-03  1.73769996e-01\n","  1.73400007e-02 -2.44890004e-01 -4.97429997e-01 -3.58109996e-02\n"," -2.54370004e-01 -6.25339970e-02  3.97289991e-01  4.95790005e-01\n"," -5.66009998e-01 -2.81610012e-01  3.46910000e-01  3.77139986e-01\n"," -2.77919993e-02 -7.74360001e-01 -3.74559999e-01 -8.21089968e-02\n","  1.92100000e+00  3.15290004e-01 -3.73369992e-01 -4.25590008e-01\n","  1.18359998e-01 -1.32200003e-01 -3.31429988e-01  4.31789994e-01\n"," -2.96979994e-02  2.93480009e-01  3.47189993e-01 -7.50899971e-01\n","  5.40799975e-01  1.60129994e-01  3.99420001e-02  3.29670012e-02\n"," -1.74160004e-01 -1.57480001e-01  2.02590004e-01 -2.43100002e-02]\n","X-----------> username sheet i can t do that too a cripple\n","{'username': 1, 'sheet': 2, 'i': 3, 'can': 4, 't': 5, 'do': 6, 'that': 7, 'too': 8, 'a': 9, 'cripple': 10}\n","Number of unique words in dictionary= 10\n","Dictionary is =  {'username': 1, 'sheet': 2, 'i': 3, 'can': 4, 't': 5, 'do': 6, 'that': 7, 'too': 8, 'a': 9, 'cripple': 10}\n","Dense vector for first word is =>  11\n","[ 2.31450006e-01  5.29619992e-01 -5.35369992e-01  9.81389999e-01\n","  8.42299998e-01 -4.74139988e-01  1.02160001e+00 -7.73060024e-02\n","  2.85290003e-01  9.78779972e-01 -1.80030003e-01  1.91689998e-01\n"," -2.27049994e+00 -3.78369987e-01 -6.54609978e-01  3.20270002e-01\n","  4.88429993e-01  5.85319996e-01 -2.28530005e-01 -1.37679994e+00\n","  4.01560009e-01  4.25909996e-01 -6.91389978e-01  1.97329998e-01\n"," -1.00929999e+00 -9.14740026e-01  4.19869989e-01 -4.62269992e-01\n","  5.75929999e-01 -2.07100004e-01 -3.77869993e-01  4.35629994e-01\n"," -1.83559999e-01 -9.21829998e-01  3.70510012e-01 -7.58870006e-01\n","  7.63230026e-01 -8.28610003e-01  4.39019985e-02 -9.01539981e-01\n"," -2.76019990e-01  7.94510022e-02  2.89550006e-01  3.91279995e-01\n"," -1.07929997e-01 -7.60680020e-01 -5.86239994e-01 -4.66710001e-01\n"," -4.14290011e-01 -1.87399998e-01 -2.92399991e-02  9.13200021e-01\n"," -8.82309973e-02 -4.87190008e-01  5.32149971e-01  6.46290004e-01\n","  8.52940008e-02 -3.51480007e-01 -1.98359994e-04  5.55739999e-02\n","  7.18429983e-01 -4.13910002e-01  2.02739999e-01 -4.96129990e-02\n"," -6.89840019e-01  6.93420023e-02 -6.56080008e-01  3.00379992e-01\n"," -2.17920005e-01  7.09220022e-02 -2.02140003e-01  5.00670016e-01\n"," -1.21339999e-01 -5.14779985e-01 -6.25800014e-01 -9.60920006e-02\n"," -6.41380012e-01 -2.73730010e-01 -7.34189987e-01  4.71179992e-01\n","  4.22719985e-01  3.05110008e-01 -9.23570022e-02 -8.03330004e-01\n"," -7.11279988e-01 -1.09089994e+00 -4.52050000e-01 -3.64809990e-01\n","  3.82279992e-01 -1.06200005e-03  2.31979992e-02  4.87140000e-01\n"," -2.07010005e-02 -2.67309994e-01  3.14260006e-01 -1.63959995e-01\n"," -1.27910003e-01  4.63389993e-01 -3.39599997e-01  1.71409994e-01]\n","X-----------> fuck ray rice  someone crack that bitch\n","{'fuck': 1, 'ray': 2, 'rice': 3, 'someone': 4, 'crack': 5, 'that': 6, 'bitch': 7}\n","Number of unique words in dictionary= 7\n","Dictionary is =  {'fuck': 1, 'ray': 2, 'rice': 3, 'someone': 4, 'crack': 5, 'that': 6, 'bitch': 7}\n","Dense vector for first word is =>  8\n","[ 1.59089997e-01 -1.46170005e-01 -9.72959995e-02  4.90590006e-01\n"," -3.39390010e-01  3.14900018e-02  4.51469980e-03  1.62249997e-01\n","  7.94849992e-02  9.12519991e-01  3.81949991e-01  1.05650000e-01\n"," -4.76170015e+00 -1.29439998e-02 -4.06659991e-02 -2.82910001e-02\n"," -1.45260006e-01  8.83409977e-02 -5.00050008e-01 -3.97619992e-01\n"," -2.84790009e-01  2.08619997e-01 -9.40710008e-02  2.95610011e-01\n","  8.30690026e-01 -9.39670026e-01  6.85580000e-02  3.73109989e-02\n"," -3.12709987e-01 -3.60700011e-01 -8.15999985e-01 -3.72570008e-01\n"," -1.42800003e-01 -4.54359986e-02 -7.10420012e-01  5.14689982e-01\n"," -5.21040022e-01  5.49409986e-01  5.68769991e-01 -2.20019996e-01\n"," -7.87689984e-01  1.77959993e-01 -2.53500015e-01 -7.03530014e-01\n","  1.99039996e-01 -9.54559967e-02  2.86020011e-01  8.27310026e-01\n","  2.31309995e-01  3.30240011e-01 -2.32370004e-01  3.32960010e-01\n","  1.54220000e-01 -1.95400000e-01  4.77730006e-01 -8.00509974e-02\n"," -7.24569976e-01 -2.25070000e-01 -2.33040005e-01  5.73339999e-01\n","  9.65600014e-02  5.45819998e-02  2.38409996e-01  3.13219994e-01\n"," -1.62990004e-01 -4.71879989e-01 -2.42180005e-01  6.12890005e-01\n","  2.78840005e-01  1.23790003e-01 -1.73260003e-01  3.73189986e-01\n","  2.84619987e-01  1.31229997e-01 -5.26340008e-01  1.98310003e-01\n","  1.00590003e+00  2.50150007e-03  5.50739979e-03  6.32990003e-01\n","  1.51619995e+00 -2.28890002e-01 -6.14960015e-01 -4.43879992e-01\n","  8.50370005e-02  1.38840005e-01 -5.02830029e-01  6.91850036e-02\n"," -3.85500014e-01 -2.71360010e-01 -7.54230022e-01  3.43520008e-02\n"," -3.88920009e-01  5.62359989e-01 -2.23820001e-01  7.65869975e-01\n"," -1.35820001e-01  2.37220004e-01  6.82550013e-01  1.16099998e-01]\n"]}],"source":["global_preprocessing(X_train)"]},{"cell_type":"code","execution_count":35,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["11696    [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...\n","4507     [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...\n","23835    [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...\n","Name: tweet, dtype: object\n"]}],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"},"vscode":{"interpreter":{"hash":"e3dfc840cebaea787b1b3c94d2ad551f7e59fd0e6203d0a4c905177db9f3c4cb"}}},"nbformat":4,"nbformat_minor":0}
